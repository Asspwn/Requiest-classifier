{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alternate-pasta",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NurimanovA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import digits \n",
    "import math\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gc\n",
    "from scipy import sparse\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291dae7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1a8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data_train.csv', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247ad2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3997\n",
       "1     106\n",
       "Name: Result, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[['subject', 'description_text', 'agent_name', 'type', 'group_name', 'Result']]\n",
    "df_train['Text'] = df_train['subject'] + df_train['description_text'] \n",
    "\n",
    "df_train.dropna(subset=['Text'], inplace=True)\n",
    "df_train.Result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf7ac9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3997\n",
       "1     106\n",
       "Name: Result, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001a96c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "banned = ['Request', 'for']\n",
    "f = lambda x: ' '.join([item for item in x.split() if item not in banned])\n",
    "df_train[\"Text\"] = df_train[\"Text\"].apply(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450966b0",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aging-bennett",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NURIMA~1\\AppData\\Local\\Temp/ipykernel_15744/1481684415.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Description'] = df1['Description'].apply(r_dup)\n",
      "C:\\Users\\NURIMA~1\\AppData\\Local\\Temp/ipykernel_15744/1481684415.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Description'] = df1['Description'].apply(r_dup)\n",
      "C:\\Users\\NURIMA~1\\AppData\\Local\\Temp/ipykernel_15744/1481684415.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Lemma'] = df1['Description'].apply(text_process)\n",
      "C:\\Users\\NURIMA~1\\AppData\\Local\\Temp/ipykernel_15744/1481684415.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Lemma'] = df1['Lemma'].apply(lemmatize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR l2 norm, balanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NURIMA~1\\AppData\\Local\\Temp/ipykernel_15744/1481684415.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Description'] = df1['Description'].apply(lemmatize)\n",
      "C:\\Users\\NURIMA~1\\AppData\\Local\\Temp/ipykernel_15744/1481684415.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['pos_teg'] = df1['Result']\n",
      "C:\\Users\\NURIMA~1\\AppData\\Local\\Temp/ipykernel_15744/1481684415.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['pos_teg']= df1['Description'].apply(pos_tag)\n"
     ]
    }
   ],
   "source": [
    "# 1 вариант объединяю имя и дескрипшн\n",
    "\n",
    "def remove(text):\n",
    "    return text[:500]\n",
    "\n",
    "df_train['Text'] = df_train['Text'].apply(remove)\n",
    "\n",
    "df1 = df_train[['Text','Result']]\n",
    "df1.columns = ['Description','Result']\n",
    "\n",
    "def r_dup(sent):\n",
    "    words = sent.split() \n",
    "    res = [] \n",
    "    for word in words: \n",
    "        # If condition is used to store unique string  \n",
    "        # in another list 'k'  \n",
    "        if (sent.count(word)>1 and (word not in res)or sent.count(word)==1): \n",
    "            res.append(word) \n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "df1['Description'] = df1['Description'].apply(r_dup)\n",
    "\n",
    "# df_1_o = df1.loc[df1.Result == 1].reset_index(drop=True)\n",
    "# df_0_o = df1.loc[df1.Result == 0].reset_index(drop=True)\n",
    "# Preprocessing\n",
    "\n",
    "#Remove words | bag of words\n",
    "\n",
    "# rubbish = ['Услуга','Услуги','Работа','Работы']\n",
    "\n",
    "def text_process(mess):\n",
    "    \n",
    "    mess = mess.replace('/', ' ')\n",
    "    mess = mess.replace('(',' ')\n",
    "    mess = mess.replace(')',' ')\n",
    "    mess = mess.replace('-',' ')\n",
    "    \n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    nopunc = [word for word in nopunc.split() if word.lower() not in stopwords.words('russian')]\n",
    "    return ' '.join(nopunc)\n",
    "\n",
    "# убрать стопвордс\n",
    "# убрать цифры\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    res = list()\n",
    "    words = text.split() # разбиваем текст на слова\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "        \n",
    "    #removing numbers    \n",
    "    res = [''.join(x for x in i if x.isalpha()) for i in res] \n",
    "    while '' in res:\n",
    "        \n",
    "        res.remove('')     \n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "def pos_tag(text):\n",
    "    words = text.split()\n",
    "    res = list()\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.tag.POS)\n",
    "    return res       \n",
    "\n",
    "#remove duplicate\n",
    "\n",
    "\n",
    "\n",
    "# df1['Lemma'] = df1['Description'].apply(r_dup)\n",
    "\n",
    "df1['Description'] = df1['Description'].apply(r_dup)\n",
    "df1['Lemma'] = df1['Description'].apply(text_process)\n",
    "df1['Lemma'] = df1['Lemma'].apply(lemmatize)\n",
    "\n",
    "print('LR l2 norm, balanced')                                \n",
    "    \n",
    "# df1['Description'] = df1['Description'].apply(text_process)                                 \n",
    "df1['Description'] = df1['Description'].apply(lemmatize)\n",
    "df1['pos_teg'] = df1['Result']\n",
    "df1['pos_teg']= df1['Description'].apply(pos_tag)                                        \n",
    "  \n",
    "\n",
    "#divide reslts\n",
    "\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df_1 = df1.loc[df1.Result == 1].reset_index(drop=True)\n",
    "df_0 = df1.loc[df1.Result == 0].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8a988",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6f37ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix Training:\n",
      " [[2918  483]\n",
      " [   3   83]]\n",
      "Accurracy Training: 0.8606251792371666\n",
      "Recall Train: 0.9651162790697675\n",
      "Confusion Matrix Testing:\n",
      " [[518  78]\n",
      " [  4  16]]\n",
      "Accurracy Test: 0.8668831168831169\n",
      "Recall Test: 0.8\n"
     ]
    }
   ],
   "source": [
    "########## LOGISTIC REGRESIION ##############\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df1['Description'],df1['Result'],test_size=0.15, random_state=np.random)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(max_features=1500,\n",
    "                             norm='l2', analyzer='char', stop_words=\"russian\", ngram_range=(2,5),dtype=np.float32)),\n",
    "    ('classifier', LogisticRegression(penalty='l2', class_weight='balanced'))\n",
    "])\n",
    "\n",
    "Y_train=Y_train.astype('int')\n",
    "Y_test=Y_test.astype('int')\n",
    "df1['Result'] = df1['Result'].astype('int')\n",
    "\n",
    "#pipeline.fit(df1['Description'], df1['Result'])\n",
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = (pipeline.predict_proba(X_train)[:,1]>=0.4).astype(float)\n",
    "Y_test_pred = (pipeline.predict_proba(X_test)[:,1]>=0.4).astype(float)\n",
    "\n",
    "cmtr = confusion_matrix(Y_train, Y_train_pred) \n",
    "print(\"Confusion Matrix Training:\\n\", cmtr) \n",
    "\n",
    "acctr = accuracy_score(Y_train, Y_train_pred) \n",
    "print(\"Accurracy Training:\", acctr) \n",
    "\n",
    "rcl = recall_score(Y_train, Y_train_pred)\n",
    "print(\"Recall Train:\", rcl) \n",
    "\n",
    "cmte = confusion_matrix(Y_test, Y_test_pred) \n",
    "print(\"Confusion Matrix Testing:\\n\", cmte) \n",
    "\n",
    "accte = accuracy_score(Y_test, Y_test_pred) \n",
    "print(\"Accurracy Test:\", accte) \n",
    "\n",
    "rcl = recall_score(Y_test, Y_test_pred)\n",
    "print(\"Recall Test:\", rcl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df411a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='char', dtype=<class 'numpy.float32'>,\n",
       "                                 max_features=1500, ngram_range=(2, 5),\n",
       "                                 stop_words='russian')),\n",
       "                ('classifier', LogisticRegression(class_weight='balanced'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## LOGISTIC REGRESIION ##############\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(df1['Description'],df1['Result'],test_size=0.2, random_state=np.random)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf',TfidfVectorizer(max_features=1500,\n",
    "                             norm='l2', analyzer='char', stop_words=\"russian\", ngram_range=(2,5),dtype=np.float32)),\n",
    "    ('classifier', LogisticRegression(penalty='l2', class_weight='balanced'))\n",
    "])\n",
    "\n",
    "Y_train=Y_train.astype('int')\n",
    "Y_test=Y_test.astype('int')\n",
    "df1['Result'] = df1['Result'].astype('int')\n",
    "\n",
    "#pipeline.fit(df1['Description'], df1['Result'])\n",
    "pipeline.fit(df1['Description'],df1['Result']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa85a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
